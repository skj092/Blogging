[
  {
    "objectID": "posts/git-for-advance/index.html",
    "href": "posts/git-for-advance/index.html",
    "title": "Mastering Git- A Comprehensive Tutorial for Moderate and Advanced Users",
    "section": "",
    "text": "Are you a developer looking to up your Git game? Look no further! In this tutorial, we’ll cover everything you need to know about using Git for version control, from the basics for those who are just starting out, to advanced techniques for more experienced users.\nFirst, let’s start with a quick overview of what Git is and why it’s so important. Git is a distributed version control system that allows developers to track changes to their codebase and collaborate with team members on projects. It helps to keep track of every modification made to the code, as well as who made those changes and when. This makes it easy to roll back changes if something goes wrong, and also allows multiple developers to work on the same project simultaneously without overwriting each other’s work.\nNow that we’ve got the basics out of the way, let’s dive into some of the key Git commands that you’ll need to know.\nBasic Git Commands * git init: This command is used to initialize a new Git repository. * git clone: This command is used to create a local copy of a remote repository. * git add: This command is used to add new files to the staging area. * git commit: This command is used to save changes to the local repository. * git push: This command is used to send your local commits to a remote repository. * git pull: This command is used to retrieve updates from a remote repository and merge them with your local repository.\nAdvanced Git Commands * git stash: This command is used to temporarily save changes that are not ready to be committed. * git branch: This command is used to create and manage branches in a Git repository. * git merge: This command is used to merge changes from one branch into another. * git rebase: This command is used to reapply commits on top of another base commit.\nHere’s an example of how you might use these commands in a real-world scenario. Let’s say you’re working on a feature for a project and you want to create a new branch for your work. You can use the git branch command to create a new branch, then use git checkout to switch to that branch.\n\n$ git branch feature-x\n$ git checkout feature-x\n\nNow, you can make changes to your code and use git add and git commit to save your progress. When you’re ready to merge your changes back into the main branch, you can use the git merge command.\n\n$ git add main.c\n$ git commit -m \"Add feature x\"\n$ git checkout master\n$ git merge feature-x\n\nThis will merge the changes from your feature-x`` branch into themaster`` branch.\nResolving conflicts\nOne thing that can happen when working with Git is that you may run into conflicts when merging branches. This can happen when multiple people have made changes to the same lines of code in different branches, and Git doesn’t know which changes to keep.\nTo resolve these conflicts, you’ll need to edit the affected files and decide which changes to keep. Then, you’ll need to use the git add command to mark the conflicts as resolved and commit the changes.\nHere’s an example of how you might do this:\n$ git merge feature-y Updating a422352..5fdff0f error: Merge conflict in main.c Auto-merging main.c CONFLICT (content): Merge conflict in main.c Automatic merge failed; fix conflicts and then commit the result.\nThis message tells you that there was a conflict when trying to merge feature-y into the current branch. You’ll need to open main.c and look for the following markers:\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nCurrent branch version\n=======\nIncoming branch version\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-y\n\nYou’ll need to decide which changes to keep and delete the conflict markers. Then, you can use git add to mark the file as resolved and commit the changes.\n\n$ git add main.c\n$ git commit -m \"Resolve conflicts\"\n\nGit aliases\nAnother useful tip is to create aliases for frequently used Git commands. This can save you time and typing by allowing you to use shorter versions of commands. For example, you can create an alias for git commit -m with the following command:\n\n$ git config --global alias.cm \"commit -m\"\n\nNow, instead of typing git commit -m \"Commit message\", you can simply use git cm \"Commit message\". You can create aliases for any Git command in this way.\nGit hooks\nGit hooks are scripts that can be used to automate tasks when certain Git events occur. For example, you can use a hook to automatically run tests before committing code, or to send a notification when a branch is pushed to a remote repository.\nThere are two types of hooks: client-side and server-side. Client-side hooks are stored in the `.git/hooks`` directory of a repository and are executed on the developer’s machine. Server-side hooks are stored on the server and are executed when certain events occur on the server, such as when a new commit is pushed.\nTo use Git hooks, you’ll need to create a script in the appropriate hook directory and make it executable. The script should contain the commands that you want to run when the hook is triggered.\nHere’s an example of a client-side pre-commit hook that runs tests before allowing a commit:\n\n#!/bin/sh\n\n# Run tests\n./run-tests.sh\n\n# Check the exit code\nif [ $? -ne 0 ]\nthen\n  echo \"Tests failed. Commit aborted.\"\n  exit 1\nfi\n\nThere are many other Git commands and techniques that we haven’t covered here, but this should give you a good foundation to start from. With practice and experience, you’ll become a Git pro in no time!"
  },
  {
    "objectID": "posts/read-better/index.html",
    "href": "posts/read-better/index.html",
    "title": "Read Efficiently",
    "section": "",
    "text": "Reading and taking notes are important skills that can help you learn and grow. By reading, you expose yourself to new ideas and perspectives, and by taking notes, you can organize and internalize this information. However, simply reading and taking notes isn’t enough - you need to do it in a way that is effective and efficient. Here are some tips for reading and taking notes in the best way to improve yourself:\n\nChoose the right material: Before you start reading, choose material that is relevant and interesting to you. This will help you stay engaged and motivated to learn. Avoid materials that are too difficult or too easy, as they may not challenge you enough or may not provide enough value.\nSet a purpose: Before you start reading, ask yourself what you hope to get out of the material. This will help you focus and stay on track as you read.\nPreview the material: Before you start reading, take a few minutes to scan the material and get an overview. Look for headings, subheadings, and highlighted words or phrases. This will give you a sense of what the material is about and help you focus on the most important points.\nTake effective notes: As you read, take notes in a way that helps you understand and remember the material. Use bullet points or numbered lists to organize your notes, and try to use your own words as much as possible. Avoid copying large chunks of text, as this can be overwhelming and may not help you internalize the information.\nReview your notes: After you finish reading, take some time to review your notes. This will help you reinforce your learning and identify any gaps in your understanding. You can also use your notes to create a summary or review sheet, which can be helpful for future reference.\n\nBy following these tips, you can read and take notes in a way that is effective and efficient, and use this information to improve yourself. Happy learning!"
  },
  {
    "objectID": "posts/birdCLEF-2024/2024-05-02-birdCLEF-2024.html",
    "href": "posts/birdCLEF-2024/2024-05-02-birdCLEF-2024.html",
    "title": "BirdCLEF-2O24",
    "section": "",
    "text": "The AIM of this post is to understand and analyze the provided data in the competitions.\n\nSo far, you must have heard about BirdCLEF 2024 competition. If not, you can check out below references to know about the Competition.\n\nKaggle BirdClif 2024\nMy Understanding of the comptition\n\n\nNote: If you want to run the notebook and experiment here is the link: BirdCLIF EDA\n\n# Import necessary libraries\nfrom fastai.vision.all import Path, get_files\nimport soundfile as sf\nimport librosa as lb\nimport librosa.display as lbd\nfrom IPython.display import Audio\nfrom soundfile import SoundFile\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport pandas as pd \nfrom fastbook import *\nfrom IPython.display import Image, display, Audio, Markdown\nimport plotly.express as px\n\n\n# Set config \nclass Config:\n    sampling_rate = 32000\n    duration = 5\n    fmin = 0\n    fmax = None\n    audios_path = Path(\"../data/train_audio\")\n    out_dir_train = Path(\"specs/train\")\n    out_dir_valid = Path(\"specs/valid\")\n\nSome utility functions\n\n# Get info of the audio file \ndef get_audio_info(filepath):\n    \"\"\"Get some properties from  an audio file\"\"\"\n    with SoundFile(filepath) as f:\n        sr = f.samplerate\n        frames = f.frames\n        duration = float(frames)/sr\n    return {\"frames\": frames, \"sr\": sr, \"duration\": duration}\n\n\n# Compute the spectogram of the audio file\ndef compute_melspec(y, sr, n_mels, fmin, fmax):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = lb.feature.melspectrogram(\n        y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,\n    )\n\n    melspec = lb.power_to_db(melspec).astype(np.float32)\n    return melspec\n\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) &gt; eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n\nsr, n_mels, fmin, fmax  = Config.sampling_rate, 128, Config.fmin, Config.fmax\ndef audio_to_image(audio):\n    melspec = compute_melspec(audio, sr=sr, n_mels = n_mels, fmin=fmin, fmax=fmax)\n    image = mono_to_color(melspec)\n    return image\n\n\npath = Path(\"../data/\")\naudio_files = get_files(path / \"train_audio\", extensions=\".ogg\")\nprint(f\"Found {len(audio_files)} audio files\")\n\nFound 24459 audio files\n\n\nSo we we 24459 audios of different length in the training data\n\nLets hear some audio and see their spectrogram to get some glimps\n\n# take a random sample\naudio_path = random.choice(audio_files)\ninfo = get_audio_info(audio_path)\nprint(info)\n\n# Convert to spectrogram\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\n\n# show spectrogra\nplt.imshow(img)\nplt.show()\n\n# play audio\ny, sr = lb.load(audio_path)\nAudio(y, rate=sr)\n\n{'frames': 2066390, 'sr': 32000, 'duration': 64.5746875}\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# take a random sample\naudio_path = random.choice(audio_files)\ninfo = get_audio_info(audio_path)\nprint(info)\n\n# Convert to spectrogram\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\n\n# show spectrogra\nplt.imshow(img)\nplt.show()\n\n# play audio\ny, sr = lb.load(audio_path)\nAudio(y, rate=sr)\n\n{'frames': 906971, 'sr': 32000, 'duration': 28.34284375}\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# take a random sample\naudio_path = random.choice(audio_files)\ninfo = get_audio_info(audio_path)\nprint(info)\n\n# Convert to spectrogram\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\n\n# show spectrogra\nplt.imshow(img)\nplt.show()\n\n# play audio\ny, sr = lb.load(audio_path)\nAudio(y, rate=sr)\n\n{'frames': 301760, 'sr': 32000, 'duration': 9.43}\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# take a random sample\naudio_path = random.choice(audio_files)\ninfo = get_audio_info(audio_path)\nprint(info)\n\n# Convert to spectrogram\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\n\n# show spectrogra\nplt.imshow(img)\nplt.show()\n\n# play audio\ny, sr = lb.load(audio_path)\nAudio(y, rate=sr)\n\n{'frames': 1496832, 'sr': 32000, 'duration': 46.776}\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# take a random sample\naudio_path = random.choice(audio_files)\ninfo = get_audio_info(audio_path)\nprint(info)\n\n# Convert to spectrogram\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\n\n# show spectrogra\nplt.imshow(img)\nplt.show()\n\n# play audio\ny, sr = lb.load(audio_path)\nAudio(y, rate=sr)\n\n{'frames': 1823232, 'sr': 32000, 'duration': 56.976}\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nYou will find that the bird sound is brighter on the spectrogram\nSince sample rate is 32000, so a 56.976s audio when loaded using python it will become a array of length 56.976 * 32000 = 1823232 which is number of frame.\nIf you want to dig deepler into how sound is represented digitally - check this blog\n\n\nLets study the metadata to get more insights\n\ndf = pd.read_csv('../data/train_metadata.csv')\ndf.shape\n\n(24459, 12)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nprimary_label\nsecondary_labels\ntype\nlatitude\nlongitude\nscientific_name\ncommon_name\nauthor\nlicense\nrating\nurl\nfilename\n\n\n\n\n0\nasbfly\n[]\n['call']\n39.2297\n118.1987\nMuscicapa dauurica\nAsian Brown Flycatcher\nMatt Slaymaker\nCreative Commons Attribution-NonCommercial-ShareAlike 3.0\n5.0\nhttps://www.xeno-canto.org/134896\nasbfly/XC134896.ogg\n\n\n1\nasbfly\n[]\n['song']\n51.4030\n104.6401\nMuscicapa dauurica\nAsian Brown Flycatcher\nMagnus Hellström\nCreative Commons Attribution-NonCommercial-ShareAlike 3.0\n2.5\nhttps://www.xeno-canto.org/164848\nasbfly/XC164848.ogg\n\n\n2\nasbfly\n[]\n['song']\n36.3319\n127.3555\nMuscicapa dauurica\nAsian Brown Flycatcher\nStuart Fisher\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0\n2.5\nhttps://www.xeno-canto.org/175797\nasbfly/XC175797.ogg\n\n\n3\nasbfly\n[]\n['call']\n21.1697\n70.6005\nMuscicapa dauurica\nAsian Brown Flycatcher\nvir joshi\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0\n4.0\nhttps://www.xeno-canto.org/207738\nasbfly/XC207738.ogg\n\n\n4\nasbfly\n[]\n['call']\n15.5442\n73.7733\nMuscicapa dauurica\nAsian Brown Flycatcher\nAlbert Lastukhin & Sergei Karpeev\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0\n4.0\nhttps://www.xeno-canto.org/209218\nasbfly/XC209218.ogg\n\n\n\n\n\n\n\n\n\ndf.primary_label.nunique()\n\n182\n\n\nThere are total 182 unique birds sounds in the competitions\n\nvalue_counts = df['primary_label'].value_counts()\n\nNumber of sample available for each bird\n\n# Plotting only the top N values\ntop_n = value_counts.head(50) # Adjust N as needed\ntop_n.plot(kind='bar', figsize=(20, 6))\n\nplt.title('Top N Value Counts of column_name')\nplt.xlabel('Unique Values')\nplt.ylabel('Counts')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plotting only the bottom N values\ntop_n = value_counts.tail(50) # Adjust N as needed\ntop_n.plot(kind='bar', figsize=(20, 6))\n\nplt.title('Top N Value Counts of column_name')\nplt.xlabel('Unique Values')\nplt.ylabel('Counts')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFor few birds 500 samples are present while for some there are only 5\n\n\nDestribution of bird on the global map\n\nfig = px.scatter_mapbox(df, lat='latitude', lon='longitude', color='primary_label', \n                        hover_name='primary_label', hover_data=['latitude', 'longitude'], \n                        title='Geographical Distribution of Bird Species',\n                        zoom=1, height=600)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.show()\n\n                                                \n\n\nData contain sound of all over the world, but there are two huge cluster on Asia and Europe.\n\n\nLets study about few birds\n\nname = \"yebbab1\"\ntemp = df.loc[df['primary_label'] == name]\nprint(f\"total number of bird in the dataset: {len(temp)}\")\n# Download some images of the bird\n\nfig = px.scatter_mapbox(temp, lat='latitude', lon='longitude', color='primary_label', \n                        hover_name='primary_label', hover_data=['latitude', 'longitude'], \n                        title='Geographical Distribution of Bird Species',\n                        zoom=1, height=600)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.show()\n\n# Assuming 'temp' is a DataFrame with bird data\nidx = random.randint(0, len(temp)-1)\nentry = temp.iloc[idx]\n\nfilename = entry['filename']\nscientific_name = entry['scientific_name']\ncommon_name = entry['common_name']\nurls = search_images_ddg(common_name, max_images=1)\n\n# Display bird information\ndisplay(Markdown(f\"### Bird Information\"))\ndisplay(Markdown(f\"**Scientific Name:** {scientific_name}\"))\ndisplay(Markdown(f\"**Common Name:** {common_name}\"))\ndisplay(Image(url=urls[0], width=300, height=300))\n\n# Audio information\naudio_path = os.path.join(Config.audios_path, filename)\ninfo = get_audio_info(audio_path)\ndisplay(Markdown(f\"### Audio Information\"))\nprint(f\"Audio Info: {info} \\n\")\n\n# Audio Spectrogram\ndisplay(Markdown(f\"### Audio Spectrogram\"))\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\nplt.imshow(img)\nplt.axis('off')  # Optional: Hide axis\nplt.show()\n\n# Play audio\ny, sr = lb.load(audio_path)\ndisplay(Audio(y, rate=sr))\n\ntotal number of bird in the dataset: 28\nAudio Info: {'frames': 354048, 'sr': 32000, 'duration': 11.064} \n\n\n\n                                                \n\n\nBird Information\n\n\nScientific Name: Argya affinis\n\n\nCommon Name: Yellow-billed Babbler\n\n\n\n\n\nAudio Information\n\n\nAudio Spectrogram\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMoipig1\n\nname = \"moipig1\"\ntemp = df.loc[df['primary_label'] == name]\nprint(f\"total number of bird in the dataset: {len(temp)}\")\n# Download some images of the bird\n\nfig = px.scatter_mapbox(temp, lat='latitude', lon='longitude', color='primary_label', \n                        hover_name='primary_label', hover_data=['latitude', 'longitude'], \n                        title='Geographical Distribution of Bird Species',\n                        zoom=1, height=600)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.show()\n\n# Assuming 'temp' is a DataFrame with bird data\nidx = random.randint(0, len(temp)-1)\nentry = temp.iloc[idx]\n\nfilename = entry['filename']\nscientific_name = entry['scientific_name']\ncommon_name = entry['common_name']\nurls = search_images_ddg(common_name, max_images=1)\n\n# Display bird information\ndisplay(Markdown(f\"### Bird Information\"))\ndisplay(Markdown(f\"**Scientific Name:** {scientific_name}\"))\ndisplay(Markdown(f\"**Common Name:** {common_name}\"))\ndisplay(Image(url=urls[0], width=300, height=300))\n\n# Audio information\naudio_path = os.path.join(Config.audios_path, filename)\ninfo = get_audio_info(audio_path)\ndisplay(Markdown(f\"### Audio Information\"))\nprint(f\"Audio Info: {info} \\n\")\n\n# Audio Spectrogram\ndisplay(Markdown(f\"### Audio Spectrogram\"))\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\nplt.imshow(img)\nplt.axis('off')  # Optional: Hide axis\nplt.show()\n\n# Play audio\ny, sr = lb.load(audio_path)\ndisplay(Audio(y, rate=sr))\n\ntotal number of bird in the dataset: 27\nAudio Info: {'frames': 589322, 'sr': 32000, 'duration': 18.4163125} \n\n\n\n                                                \n\n\nBird Information\n\n\nScientific Name: Ducula badia\n\n\nCommon Name: Mountain Imperial-Pigeon\n\n\n\n\n\nAudio Information\n\n\nAudio Spectrogram\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nname = \"integr\"\ntemp = df.loc[df['primary_label'] == name]\nprint(f\"total number of bird in the dataset: {len(temp)}\")\n# Download some images of the bird\n\nfig = px.scatter_mapbox(temp, lat='latitude', lon='longitude', color='primary_label', \n                        hover_name='primary_label', hover_data=['latitude', 'longitude'], \n                        title='Geographical Distribution of Bird Species',\n                        zoom=1, height=600)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.show()\n\n# Assuming 'temp' is a DataFrame with bird data\nidx = random.randint(0, len(temp)-1)\nentry = temp.iloc[idx]\n\nfilename = entry['filename']\nscientific_name = entry['scientific_name']\ncommon_name = entry['common_name']\nurls = search_images_ddg(common_name, max_images=1)\n\n# Display bird information\ndisplay(Markdown(f\"### Bird Information\"))\ndisplay(Markdown(f\"**Scientific Name:** {scientific_name}\"))\ndisplay(Markdown(f\"**Common Name:** {common_name}\"))\ndisplay(Image(url=urls[0], width=300, height=300))\n\n# Audio information\naudio_path = os.path.join(Config.audios_path, filename)\ninfo = get_audio_info(audio_path)\ndisplay(Markdown(f\"### Audio Information\"))\nprint(f\"Audio Info: {info} \\n\")\n\n# Audio Spectrogram\ndisplay(Markdown(f\"### Audio Spectrogram\"))\naudio, sr = sf.read(audio_path)\nimg = audio_to_image(audio)\nplt.imshow(img)\nplt.axis('off')  # Optional: Hide axis\nplt.show()\n\n# Play audio\ny, sr = lb.load(audio_path)\ndisplay(Audio(y, rate=sr))\n\ntotal number of bird in the dataset: 5\nAudio Info: {'frames': 150465, 'sr': 32000, 'duration': 4.70203125} \n\n\n\n                                                \n\n\nBird Information\n\n\nScientific Name: Ardea intermedia\n\n\nCommon Name: Intermediate Egret\n\n\n\n\n\nAudio Information\n\n\nAudio Spectrogram\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/git/index.html",
    "href": "posts/git/index.html",
    "title": "Getting Started with Git- A Step-by-Step Tutorial",
    "section": "",
    "text": "Are you new to version control and wondering what all the fuss is about? Git is a version control system that allows you to track changes to your code and collaborate with others on software projects. In this article, we’ll go over the basics of git and how it can help streamline your workflow using Python.\nImagine you’re working on a Python project with a team of developers. You’re all making changes to the codebase, and things can get confusing fast. Without a way to track these changes, it can be difficult to know who made what change and when. This is where git comes in.\nTo start using git, you’ll need to set up a repository. A repository is a central location where all the code and changes for your project are stored. You can create a repository on your local machine or on a remote server like GitHub. Here’s how to create a repository on your local machine using the command line:\n$ git init\nOnce you have a repository set up, you’ll need to “commit” your changes. A commit is a snapshot of your code at a specific point in time. When you make a commit, you’ll also include a commit message that explains what changes you made. This helps you and your teammates understand what was changed and why. Here’s how to make a commit in git:\n$ git add .\n$ git commit -m \"Commit message goes here\"\nYou can also create branches in your repository. A branch is a copy of your code that you can work on without affecting the main codebase. This is useful when you’re working on a new feature or trying out an experimental change. When you’re finished with your branch, you can merge it back into the main codebase. Here’s how to create and switch to a new branch in git:\n$ git branch new_branch\n$ git checkout new_branch\nGit also has the ability to resolve conflicts. If two people make changes to the same part of the code, git will alert you to a conflict. You’ll then need to manually resolve the conflict by deciding which changes to keep and which to discard. Here’s an example of how to resolve a conflict in git:\n$ git merge other_branch\nIf a conflict is detected, git will show you the conflicting files and mark the conflicting lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. You’ll need to edit the files and remove the conflict markers, keeping the changes that you want to keep and discarding the others. Then, you can add and commit the resolved files to finish merging the branches.\nGit may seem intimidating at first, but it’s a powerful tool that can greatly improve your workflow. Whether you’re working on a small Python project or a large software development team, git can help you keep track of changes and collaborate with others more efficiently."
  },
  {
    "objectID": "posts/git/index.html#working-on-some-remote-git-repository",
    "href": "posts/git/index.html#working-on-some-remote-git-repository",
    "title": "Getting Started with Git- A Step-by-Step Tutorial",
    "section": "Working on some remote git repository",
    "text": "Working on some remote git repository\nHere are the basic steps you can follow to work on a remote git repository:\n\nInstall git on your local machine: Before you can start working with a remote repository, you’ll need to have git installed on your local machine. You can download and install git from the official website (https://git-scm.com/) or through your operating system’s package manager.\nClone the repository: To work with a remote repository, you’ll first need to create a local copy of the repository on your machine. This is called “cloning” the repository. To clone a repository, you’ll need to know the repository’s URL. You can usually find the URL on the repository’s website or by looking for the “Clone or download” button on the repository’s page.\n\nTo clone the repository, open a terminal or command prompt and navigate to the directory where you want to store the local copy of the repository. Then, run the following command:\n\n$ git clone repository_url\n\nReplace “repository_url” with the actual URL of the repository. This will create a new directory with the same name as the repository and download all the files and directories from the remote repository into it.\n\nCreate a new branch: It’s a good practice to create a new branch for your changes, rather than making changes directly to the main branch of the repository. This way, you can keep your changes separate from the main codebase and easily switch between different versions of the code.\n\nTo create a new branch, navigate to the local copy of the repository on your machine and run the following command:\n\n$ git branch new_branch\n$ git checkout new_branch\n\nReplace “new_branch” with the name you want to give to your branch. This will create a new branch with the specified name and switch to it.\n\nMake and commit your changes: Once you’re on your new branch, you can start making changes to the code. When you’re finished with your changes, you’ll need to “commit” them to the repository. A commit is a snapshot of your code at a specific point in time, and it allows you to track your changes and roll back to previous versions if necessary.\n\nTo commit your changes, run the following commands:\n\n$ git add .\n$ git commit -m \"Commit message goes here\"\n\nThe first command, git add, stages your changes for commit. The second command, git commit, creates a commit with your staged changes and the specified commit message.\n\nPush your changes to the remote repository: Once you’ve committed your changes, you’ll need to push them to the remote repository so that others can see and work with them. To push your changes, run the following command:\n\n\n$ git push origin new_branch\n\nReplace “new_branch” with the name of the branch you want to push. This will send your changes to the remote repository and make them available to others."
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "The goal of this competition is to detect and classify seizures and other types of harmful brain activity. You will develop a model trained on electroencephalography (EEG) signals recorded from critically ill hospital patients.\n\n\nIn dataset provided in the competition are listed below:\n\ntrain_eegs(directory): Contain lots of .parquet files.\ntest_eegs(directory): Contain 1 .parquet file.\ntrain_spectrograms(directory): Contain lots of .parquet files.\ntest_spectrograms(directory): Contain 1 .parquet file.\ntrain.csv(file): Metadata for the training data.\nsample_submission.csv(file):\ntest.csv(file): Metadata for the test data.\nexample_figures:\n\n\n\n\nEach row of train.csv is a specific window in time (for a specific patient_id), each row has a specific middle timestamp in seconds.\nFor example row 235 has center timestamp T = May 3 2023 19:30:06 exactly in the middle of both its EEG time window and Spectrogram time window. \nThe EEG time window is length 50 seconds and the Spectrogram time window is length 600 seconds. And both have the same center timestamp. In this competition, we are asked to predict the event occurring in the middle 10 seconds of both these time windows.\nCenter is T\nEEG time window is from T-25 to T+25\nSpectrogram time window is from T-300 to T+300\nWe predict event in the middle 10 seconds of both these time windows. [T-5 to T+5]\n\n\n\n\nEEG and Spectrogram\n\n\n\n\n\nThe EEG parquet files are longer than 50 seconds. One EEG parquet file has multiple rows of time windows inside it. Similarly, the Spectrogram parquet files are longer than 600 seconds. One Spectrogram parquet file has multiple rows of time windows inside it. \n\n\n\nThere are fewer Spectrogram parquet files than EEG parquet files. This is because two rows may have the same Spectrogram parquet file but different EEG parquet files. \n\n\n\nWe have given 20 eeg time series signals for each time window. \n\n\n\n\nFor a specific row of train.csv, here is the code to retrieve the corresponding EEG and Spectrogram. Note that train.csv doesn’t give us the middle timestamp. Instead, it gives us the beginning timestamp for each time window. The two beginning are determined from eeg_label_offset_seconds and spectrogram_label_offset_seconds. These are offsets from the start of the parquet file which tells us where the EEG and spectrogram time windows begin respectively.\nGET_ROW = 0\nEEG_PATH = 'train_eegs/'\nSPEC_PATH = 'train_spectrograms/'\n\ntrain = pd.read_csv('train.csv')\nrow = train.iloc[GET_ROW]\n\neeg = pd.read_parquet(f'{EEG_PATH}{row.eeg_id}.parquet')\neeg_offset = int( row.eeg_label_offset_seconds )\neeg = eeg.iloc[eeg_offset*200:(eeg_offset+50)*200]\n\nspectrogram = pd.read_parquet(f'{SPEC_PATH}{row.spectrogram_id}.parquet')\nspec_offset = int( row.spectrogram_label_offset_seconds )\nspectrogram = spectrogram.loc[(spectrogram.time&gt;=spec_offset)\n                     &(spectrogram.time&lt;spec_offset+600)]\n\n\n\n\n\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. It is a three-dimensional plot where the x-axis represents time, the y-axis represents frequency, and the intensity of the color or brightness represents the magnitude of the frequencies present at each point in time.\n\n\n\nEEG stands for Electroencephalography. It is a technique used to record electrical activity in the brain. EEG measures the electrical potentials generated by neurons within the brain, providing valuable information about brain function and activity.\n\n\nInternational adopted method used to measure EEG by placing the electrode over the head at specific positions mentioned in the diagram.\n\n\n\n10–20 system (EEG)\n\n\n\n\n\nEEG is recorded using a differential amplifier, it provides the difference between the two nearest signals.\n\n\n\nEEG is displayed using Montages. To display it, you find the difference between two adjacent nodes (called channels). Place a few channels together in a single frame called Chain.\n\n\n\n\n\nWe are using The Bipolar Double Banana Montage method to make a spectrogram from the EEG.\n\n\n\nEEG Parquet CSV and Spectrogram Parquet CSV\n\n\nIn the diagram, we see which electrode signals are needed to make the LL, LP, RP, RR spectrograms.\n\n\n\n\nWhat is EEG\nEEGS 10–20 system\nHow to make spectrogram from EEG\nMontages & Technicalities"
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#understanding-input-data",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#understanding-input-data",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "In dataset provided in the competition are listed below:\n\ntrain_eegs(directory): Contain lots of .parquet files.\ntest_eegs(directory): Contain 1 .parquet file.\ntrain_spectrograms(directory): Contain lots of .parquet files.\ntest_spectrograms(directory): Contain 1 .parquet file.\ntrain.csv(file): Metadata for the training data.\nsample_submission.csv(file):\ntest.csv(file): Metadata for the test data.\nexample_figures:\n\n\n\n\nEach row of train.csv is a specific window in time (for a specific patient_id), each row has a specific middle timestamp in seconds.\nFor example row 235 has center timestamp T = May 3 2023 19:30:06 exactly in the middle of both its EEG time window and Spectrogram time window. \nThe EEG time window is length 50 seconds and the Spectrogram time window is length 600 seconds. And both have the same center timestamp. In this competition, we are asked to predict the event occurring in the middle 10 seconds of both these time windows.\nCenter is T\nEEG time window is from T-25 to T+25\nSpectrogram time window is from T-300 to T+300\nWe predict event in the middle 10 seconds of both these time windows. [T-5 to T+5]\n\n\n\n\nEEG and Spectrogram\n\n\n\n\n\nThe EEG parquet files are longer than 50 seconds. One EEG parquet file has multiple rows of time windows inside it. Similarly, the Spectrogram parquet files are longer than 600 seconds. One Spectrogram parquet file has multiple rows of time windows inside it. \n\n\n\nThere are fewer Spectrogram parquet files than EEG parquet files. This is because two rows may have the same Spectrogram parquet file but different EEG parquet files. \n\n\n\nWe have given 20 eeg time series signals for each time window."
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#code-to-retrieve-eeg-and-spectrogram",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#code-to-retrieve-eeg-and-spectrogram",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "For a specific row of train.csv, here is the code to retrieve the corresponding EEG and Spectrogram. Note that train.csv doesn’t give us the middle timestamp. Instead, it gives us the beginning timestamp for each time window. The two beginning are determined from eeg_label_offset_seconds and spectrogram_label_offset_seconds. These are offsets from the start of the parquet file which tells us where the EEG and spectrogram time windows begin respectively.\nGET_ROW = 0\nEEG_PATH = 'train_eegs/'\nSPEC_PATH = 'train_spectrograms/'\n\ntrain = pd.read_csv('train.csv')\nrow = train.iloc[GET_ROW]\n\neeg = pd.read_parquet(f'{EEG_PATH}{row.eeg_id}.parquet')\neeg_offset = int( row.eeg_label_offset_seconds )\neeg = eeg.iloc[eeg_offset*200:(eeg_offset+50)*200]\n\nspectrogram = pd.read_parquet(f'{SPEC_PATH}{row.spectrogram_id}.parquet')\nspec_offset = int( row.spectrogram_label_offset_seconds )\nspectrogram = spectrogram.loc[(spectrogram.time&gt;=spec_offset)\n                     &(spectrogram.time&lt;spec_offset+600)]"
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#keywords-and-their-meanings",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#keywords-and-their-meanings",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. It is a three-dimensional plot where the x-axis represents time, the y-axis represents frequency, and the intensity of the color or brightness represents the magnitude of the frequencies present at each point in time.\n\n\n\nEEG stands for Electroencephalography. It is a technique used to record electrical activity in the brain. EEG measures the electrical potentials generated by neurons within the brain, providing valuable information about brain function and activity.\n\n\nInternational adopted method used to measure EEG by placing the electrode over the head at specific positions mentioned in the diagram.\n\n\n\n10–20 system (EEG)\n\n\n\n\n\nEEG is recorded using a differential amplifier, it provides the difference between the two nearest signals.\n\n\n\nEEG is displayed using Montages. To display it, you find the difference between two adjacent nodes (called channels). Place a few channels together in a single frame called Chain."
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#create-spectrogram-from-eeg",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#create-spectrogram-from-eeg",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "We are using The Bipolar Double Banana Montage method to make a spectrogram from the EEG.\n\n\n\nEEG Parquet CSV and Spectrogram Parquet CSV\n\n\nIn the diagram, we see which electrode signals are needed to make the LL, LP, RP, RR spectrograms."
  },
  {
    "objectID": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#references",
    "href": "posts/kaggle-Harmful-Brain-Activity-Classification/index.html#references",
    "title": "HMS - Harmful Brain Activity Classification",
    "section": "",
    "text": "What is EEG\nEEGS 10–20 system\nHow to make spectrogram from EEG\nMontages & Technicalities"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another data science student’s blogs",
    "section": "",
    "text": "BirdCLEF-2O24\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBirdCLEF 2024\n\n\n\n\n\nMy Understanding of the comptition so far\n\n\n\n\n\nApr 19, 2024\n\n\nSonu Jha\n\n\n\n\n\n\n\n\n\n\n\n\nHMS - Harmful Brain Activity Classification\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nSonu\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Docker - A Beginner’s Guide\n\n\nA step-by-step tutorial on how to build and run your applications using Docker containers.\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRead Efficiently\n\n\nBest way to read and take notes to improve yourself\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Git- A Comprehensive Tutorial for Moderate and Advanced Users\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBlur the face in an image or video\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Git- A Step-by-Step Tutorial\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of Artificial Neural Network(ANN)\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I’m Sonu Jha, an AI Engineer with a passion for leveraging cutting-edge technology to solve complex problems. With a background in Mathematics and extensive experience in Machine Learning and Computer Vision, I’m constantly exploring the frontiers of Artificial Intelligence."
  },
  {
    "objectID": "about.html#professional-journey",
    "href": "about.html#professional-journey",
    "title": "About Me",
    "section": "Professional Journey",
    "text": "Professional Journey\nCurrently, I’m working as an AI Engineer at Signzy Technology in Bangalore, where I’ve had the opportunity to work on exciting projects such as:\n\nDeveloping an Aadhaar Masker using advanced image segmentation techniques\nCreating a forgery detection system for identity documents\nImplementing salary slip extraction using Langchain and GenerativeAI\nBuilding a signature verification model using Siamese Architecture\n\nPrior to this, I worked as an Associate Software Developer at Accenture Solution in Mumbai, where I honed my skills in data analysis and web development."
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "About Me",
    "section": "Technical Expertise",
    "text": "Technical Expertise\nMy expertise lies in:\n\nMachine Learning and Deep Learning (PyTorch, Keras, FastAI, Hugging Face)\nComputer Vision tasks (Image Classification, Object Detection, Image Segmentation)\nNatural Language Processing (NLP) and Understanding (NLU)\nAPI development (FastAPI, Falcon, Node.js, Javascript)\nDevOps tools (Docker, CI/CD, Grafana, Kibana)\n\nI’m particularly excited about the potential of Generative AI and have been integrating tools like Langchain, GPT, and Bard to improve AI microservices."
  },
  {
    "objectID": "about.html#achievements-and-interests",
    "href": "about.html#achievements-and-interests",
    "title": "About Me",
    "section": "Achievements and Interests",
    "text": "Achievements and Interests\nI’m proud to have secured a bronze medal in the UBC Ovarian Cancer Subtype Classification competition on Kaggle. I’ve also been recognized for my work in improving AI service accuracy through the integration of Generative AI at Signzy Technology.\nBeyond the world of AI, I have a keen interest in mountaineering and have completed a Basic Mountaineering Course from the National Institute of Mountaineering and Adventure Sports."
  },
  {
    "objectID": "about.html#sharing-knowledge",
    "href": "about.html#sharing-knowledge",
    "title": "About Me",
    "section": "Sharing Knowledge",
    "text": "Sharing Knowledge\nI believe in the power of education and enjoy sharing my knowledge. I’ve had the privilege of being invited as a guest lecturer to present on “Application of Mathematics in Machine Learning” at R.D National College in Mumbai.\nThrough this blog, I aim to share my insights, experiences, and learnings in the field of AI and technology. Whether you’re a fellow AI enthusiast, a student of technology, or simply curious about the world of artificial intelligence, I hope you’ll find something valuable here.\nFeel free to connect with me on LinkedIn or check out my projects on GitHub. Let’s explore the exciting world of AI together!"
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#provided-data",
    "href": "posts/Kaggle-birdclef/index.html#provided-data",
    "title": "BirdCLEF 2024",
    "section": "Provided Data",
    "text": "Provided Data\n\ntrain_audios: Contain 182 folders, each folder contain multiple audio files of particular bird species.\ntrain_metadata.csv: Contain metadata of each audio file.\nunlabeled_soundscapes: Contain multiple audio files of bird sounds.\neBIrd_Taxonomy_v2021.csv: Data on the relationships between different species.\ntest_soundscapes: Only the hidden rerun copy of the test soundscape dirctory will only be populated.\nSampleSubmission.csv: A sample submission file in the correct format. First column is the filename, and remaining are scores for each bird species."
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#evaluation-metric",
    "href": "posts/Kaggle-birdclef/index.html#evaluation-metric",
    "title": "BirdCLEF 2024",
    "section": "Evaluation Metric",
    "text": "Evaluation Metric\n\nA version of macro-averaged ROC-AUC that skips classes which have no true positive labels."
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#dataset-info",
    "href": "posts/Kaggle-birdclef/index.html#dataset-info",
    "title": "BirdCLEF 2024",
    "section": "Dataset Info",
    "text": "Dataset Info\n\ntrain audio are in different length and sample rate is 32000\nIf an audio is for 6 second, when you read with soundfile, it will be 6*32000 = 192000 samples\nSo the size of array will be (192000,)\nSince we have to predict for each 5 second, we will split the audio into 5 second.\nNow, will select one of the 5 second audio and convert it into melspectrogram of shape (3, 128, 201)\nWhen we make batch of it the shape will be (bs, 3, 128, 201)\nModel will give output of shape (bs, num_classes), i.e. (bs, 182)"
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#status",
    "href": "posts/Kaggle-birdclef/index.html#status",
    "title": "BirdCLEF 2024",
    "section": "Status",
    "text": "Status\n\nSetup local evaluation for the competition metric\nDo more EDA on the available data\nApply augmentation in the next submission."
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#what-is-an-audio",
    "href": "posts/Kaggle-birdclef/index.html#what-is-an-audio",
    "title": "BirdCLEF 2024",
    "section": "1. What is an audio?",
    "text": "1. What is an audio?\nTo digitize a sound wave we must turn the signal into a series of numbers so that we can input it into our models. This is done by measuring the amplitude of the sound at fixed intervals of time.\n\nEach such measurement is called a sample, and the sample rate is the number of samples per second. For instance, a common sampling rate is about 44,100 samples per second. That means that a 10-second music clip would have 441,000 samples!"
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#data-preprocessing",
    "href": "posts/Kaggle-birdclef/index.html#data-preprocessing",
    "title": "BirdCLEF 2024",
    "section": "1. Data Preprocessing",
    "text": "1. Data Preprocessing\n\nRead the meta data dataframe\nSplit the data into train and valid\nAdd a column for path of the audio file\nUse this dataframe to convert audio to image"
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#convert-audio-to-image",
    "href": "posts/Kaggle-birdclef/index.html#convert-audio-to-image",
    "title": "BirdCLEF 2024",
    "section": "2. Convert Audio to Image",
    "text": "2. Convert Audio to Image\n\nRead an audio file using soundfile.read method\nChop the audio file into multiple audio files of 5 second each\nConvert all the audio files to spectrogram\nStack all the images using numpy\nSave the image to disk"
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#creating-dataset-and-dataloader",
    "href": "posts/Kaggle-birdclef/index.html#creating-dataset-and-dataloader",
    "title": "BirdCLEF 2024",
    "section": "3. Creating Dataset and Dataloader",
    "text": "3. Creating Dataset and Dataloader\n\nUse train and valid dataframe to create a dataset\nRead the row of the dataframe\nLoad the corresponding saved numpy file with custom max number of images\nWhile training select one of the image randomly\nConvert to tensor, augmentate it\nStack 2 duplicate images to make it 3 channel\nNormalize the image\nCreate a batch of 64 images to pass it to model."
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#model-and-training",
    "href": "posts/Kaggle-birdclef/index.html#model-and-training",
    "title": "BirdCLEF 2024",
    "section": "4. Model and Training",
    "text": "4. Model and Training\n\nUsing tf_efficientnet_b0_ns model from timm\nTake a batch of images and target.\nCalculate cross entropy loss and iterate to train."
  },
  {
    "objectID": "posts/Kaggle-birdclef/index.html#resources",
    "href": "posts/Kaggle-birdclef/index.html#resources",
    "title": "BirdCLEF 2024",
    "section": "5. Resources",
    "text": "5. Resources\n\nConvert Audio To Spectrogram: https://www.kaggle.com/code/nischaydnk/split-creating-melspecs-stage-1\nPytorch Lightning Inferece: https://www.kaggle.com/code/nischaydnk/birdclef-2023-pytorch-lightning-inference\nPytorch LIghtning training: https://www.kaggle.com/code/nischaydnk/birdclef-2023-pytorch-lightning-training-w-cmap\nAudio Deep Learning: https://www.kaggle.com/competitions/birdclef-2024/discussion/491668"
  },
  {
    "objectID": "posts/detect-face-and-blur/2022-05-24-Detect-Face-And-Blur.html",
    "href": "posts/detect-face-and-blur/2022-05-24-Detect-Face-And-Blur.html",
    "title": "Blur the face in an image or video",
    "section": "",
    "text": "Blurring the face area of people from videos is done in all news channels and to hide the identity of a person. With computer vision, We can automatically detect the face region of the person and use it to blur the image. In this project we will build a computer vision model which can detect face in an image and blur it.\n\n1. Read an image\n2. Convert it to grayscale\n3. load Cascade Classifier\n4. Detect Faces\n5. Draw Bounding Box\n6. Blur the face\n\n\nCode\n# Import necessary library\nimport cv2\nimport matplotlib.pyplot as plt\n\n\nStep-1: Read an image\n\n\nCode\nimg = cv2.imread('../images/face.jpg')\nplt.imshow(img[:,:,::-1]) # opencv read an image into BGR mode, to convert it into RGB we reverse the image array\n\n\n\n\n\n\n\n\n\nStep-2: Convert the image to grayscale\n\n\nCode\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray, cmap='gray')\n\n\n\n\n\n\n\n\n\nStep-3: Loading OpenCV CascadeClassifier\n\n\nCode\nface_classifier = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n\n\nStep-4: Detecting faces in the grayscaled image\n\n\nCode\nfaces = face_classifier.detectMultiScale(gray)\n\n\nStep-5: Draw Blounding Box around the detected faces\n\n\nCode\nfor (x,y,w,h) in faces:\n    cv2.rectangle(img, (x,y), (x+w,y+h), (127,0,255),2)\n    plt.imshow(img[:,:,::-1])\n\n\n\n\n\n\n\n\n\nStep-6: Blur the face\n\n\nCode\n# select region of face in the original image\nROI = img[y:y+h, x:x+w]\n\n# blur the face region\nblur_face = cv2.GaussianBlur(ROI, (91,91),0)\n\n\n\n\nCode\n# replace original face with the blurred one\nimg[y:y+h, x:x+w] = blur_face\n\n\n\n\nCode\n# Final Image\nplt.imshow(img[:,:,::-1])"
  },
  {
    "objectID": "posts/docker/index.html",
    "href": "posts/docker/index.html",
    "title": "Getting Started with Docker - A Beginner’s Guide",
    "section": "",
    "text": "Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.\nTo start using Docker, you will first need to install it on your system. You can find installation instructions for various operating systems on the Docker website.\nOnce Docker is installed, you can start using it to build and run your applications. The first step is to create a Dockerfile, which is a text file that contains all of the instructions needed to build an image.\nHere’s an example Dockerfile for a simple Python application:\n\nFROM python:3\n\nCOPY . /app\nWORKDIR /app\n\nRUN pip install -r requirements.txt\n\nCMD [\"python\", \"app.py\"]\n\nThis Dockerfile starts with the FROM directive, which specifies the base image that the rest of the instructions will be built on top of. In this case, we’re using a base image that contains the latest version of Python 3.\nNext, the COPY and WORKDIR directives are used to copy the contents of the current directory into the /app directory in the container and set that as the working directory.\nThe RUN directive is used to run a command in the container, in this case installing the dependencies specified in the requirements.txt file.\nFinally, the CMD directive is used to specify the command that will be run when the container is started. In this case, we’re running the app.py Python script.\nTo build an image based on this Dockerfile, you can use the docker build command. For example:\n\n$ docker build -t my-app .\n\nThis will build an image with the tag my-app based on the instructions in the Dockerfile.\nOnce the image is built, you can use the docker run command to start a container based on that image. For example:\n\n$ docker run -p 8000:8000 my-app\n\nThis will start a container based on the my-app image and map the container’s port 8000 to the host’s port 8000, so that the application can be accessed on http://localhost:8000.\nThere are many more options and directives that you can use in a Dockerfile and with the docker command, but this should give you a basic understanding of how to use Docker to build and run a simple application."
  },
  {
    "objectID": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#artificial-intelligence",
    "href": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#artificial-intelligence",
    "title": "Evolution of Artificial Neural Network(ANN)",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\nArtificial intelligence (AI) is a branch of computer science capable of performing tasks that typically require human intelligence."
  },
  {
    "objectID": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#what-is-deep-learning",
    "href": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#what-is-deep-learning",
    "title": "Evolution of Artificial Neural Network(ANN)",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\n\ndeep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow.\n\n\nMachine learning is the science of getting computers to act without being explicitly programmed."
  },
  {
    "objectID": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#ann-artificial-neural-networks",
    "href": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#ann-artificial-neural-networks",
    "title": "Evolution of Artificial Neural Network(ANN)",
    "section": "ANN: Artificial Neural Networks",
    "text": "ANN: Artificial Neural Networks\n\nANNs are inspired by biological neurons found in cerebral cortex of our brain.\n\n\n\nA neuron or nerve cell is an electrically excitable cell that communicates with other cells via specialized connections called synapses.\nANNs are core of deep learning. Hence one of the most important topic to understand."
  },
  {
    "objectID": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#perceptron",
    "href": "posts/ANN/2020-07-05-Evolution-of-Artificial-Neural-Network(ANN).html#perceptron",
    "title": "Evolution of Artificial Neural Network(ANN)",
    "section": "Perceptron",
    "text": "Perceptron\nA Perceptron is one of the simplese ANN architectures, invented in 1957 by Frank Rosenblatt.\n\nA perceptron works similar to a biological neuron. A biological neuron receives electrical signals from its dendriles, modulates the electrical signals in various amounts, and then fires an output signal through its synapses only when the total strength of the input signals exceeds a certain threshold. The output is then fed to another neuron, and so forth.\n\n\nTo model the biological phenomenon, the artificial neuron performs two consecutive functions: it calculates the weighted sum of the inputs to represent the totgal strength of the input signals, and it applies a step function to the result to determine whether to fire the ourput 1 if the signal exceeds a certain threshold of 0 if the signal doesn’t exceed the threshold.\n\n\nHow Peceptron Works?\nThe perceptron’s learning logic goes like this:\n\nThe neuron calculate the weighted sum and applies the activation function to make a prediction y^. This is called the feedforward process:\n\n\\[y\\hat{} =  activation\\left(\\sum\\nolimits x_i \\cdot w_i + b\\right)\\]\n\nIt compares the output prediction with the correct label to calculate the error:\n\n\\[error = y-y\\hat{}\\]\n\nIt then update the weight. If the prediction is too high, it adjust the weight to make a lower prediction the next time, and vice versa.\nRepeat!\n\nHere’s how a since perceptron works to classify two classes :\n\nDrawback of perceptron. Sometimes its not possible to get desired result with only perceptron. In the below example you can see the model in not able to draw a line to classify the data(linearly inseperable data)\n\nIndroduction of ANN If you increase the number of neuron then you cann see model works pretty well. The stack of more than one neuron is called Multi Layer Perceptron or ANN.\n\nANN using Keras\n\n\nCode\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nfrom tensorflow.keras import initializers\nfrom tensorflow.python.keras import activations\n\nprint(tf.__version__)\n\n# downloading fashion_mnist data\nfashion_mnist = tf.keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images / 255.0\n\ntest_images = test_images / 255.0 \n\nactivation = tf.keras.activations.relu\n\nmodel = tf.keras.Sequential([\ntf.keras.layers.Flatten(input_shape=(28, 28)),\ntf.keras.layers.Dense(128, activation=activation),\ntf.keras.layers.Dense(10)\n])\n\nmodel.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n\n# model summary\nmodel.summary()\n\n\nc:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\nc:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\nc:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n\n\n2.5.0\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               100480    \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nCode\nmodel.fit(train_images, train_labels, epochs=10)\n\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n\nprint('test_loss', test_loss)\nprint('test_accuracy', test_acc)\n\n\nEpoch 1/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.5000 - accuracy: 0.8255\nEpoch 2/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.3751 - accuracy: 0.8645\nEpoch 3/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3388 - accuracy: 0.8773\nEpoch 4/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3142 - accuracy: 0.8843\nEpoch 5/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2957 - accuracy: 0.8922\nEpoch 6/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2797 - accuracy: 0.8971\nEpoch 7/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2685 - accuracy: 0.8991\nEpoch 8/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.2577 - accuracy: 0.9028\nEpoch 9/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.2476 - accuracy: 0.9076\nEpoch 10/10\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.2388 - accuracy: 0.9105\n313/313 - 0s - loss: 0.3403 - accuracy: 0.8798\ntest_loss 0.34025073051452637\ntest_accuracy 0.879800021648407"
  }
]